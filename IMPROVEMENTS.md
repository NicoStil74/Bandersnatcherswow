# TUMSearch PageRank Explorer - Improvements Checklist

## ğŸ”´ Critical Bugs

- **`src/run_pagerank.py` has syntax error** â€” Contains stray shell command `python src/run_pagerank.py` as Python code, causing script to crash â†’ Remove the invalid line and test the script runs correctly

- **`frontend/src/App.test.js` is broken** â€” Tests for "learn react" text that doesn't exist in the app â†’ Rewrite tests to actually test PageRank calculation and graph rendering

## ğŸŸ¡ Code Cleanup

- **Duplicate PageRank implementations** â€” Same algorithm exists in 5 different files across Python and JavaScript â†’ Keep one Python version (`pagerank_calc.py`) and one JS version (`tim_backend/pagerank.js`), delete the rest

- **Duplicate `graph.json` files** â€” Identical file exists in `src/graph_sources/` and `frontend/src/` â†’ Keep only in `src/graph_sources/` and have frontend fetch from backend

- **Two separate backends doing the same thing** â€” `server.js` (root) and `tim_backend/server.js` both serve PageRank API â†’ Pick one backend, delete the other, update all references

- **Empty placeholder files exist** â€” `main.py`, `graph_loader.py`, `models/graph_models.py` are empty; `visualizer/` folder is empty â†’ Either implement them or delete them entirely

## ğŸŸ  Missing Integration

- **Frontend doesn't use the backend** â€” `handleCompute()` in `App.js` has `// TODO: backend call here` and computes PageRank locally instead â†’ Connect frontend to backend API for PageRank computation

- **No way to trigger crawler from UI** â€” User cannot initiate a new crawl from the frontend â†’ Add "Start Crawl" button that calls backend which runs Python crawler

- **Backend `/upload-graph` endpoint is unused** â€” API exists but nothing calls it â†’ Either use it from frontend file upload or remove it

## ~~ğŸ•·ï¸ Crawler Issues~~ âœ… DONE

- ~~**Sequential requests make it slow** â€” Crawling 100 pages takes ~73 seconds because requests are made one at a time â†’ Use `asyncio` + `aiohttp` or `concurrent.futures.ThreadPoolExecutor` for parallel fetching (could be 5-10x faster)~~ âœ…

- ~~**Using list instead of deque** â€” `to_visit.pop(0)` is O(n) operation, shifts entire list each time â†’ Use `collections.deque` with `popleft()` for O(1) performance~~ âœ…

- ~~**Only extracts links from `<main>` tags** â€” `soup.select("main a[href]")` misses navigation, footer, and sidebar links â†’ Change to `soup.select("a[href]")` or make selector configurable~~ âœ…

- ~~**No retry logic for failed requests** â€” If a page times out once, it's silently skipped forever â†’ Add 2-3 retries with exponential backoff before giving up~~ âœ…

- ~~**Weak URL normalization** â€” Doesn't handle trailing slashes, `www` vs non-`www`, or URL encoding differences â†’ Normalize all URLs properly so `tum.de/page` and `tum.de/page/` are treated as same~~ âœ…

- ~~**No depth limit** â€” Crawler can go infinitely deep into archive sections â†’ Add configurable max depth (e.g., stop at 3-4 clicks from start)~~ âœ…

- ~~**No logging** â€” Uses only `print()` statements, no structured logging â†’ Use Python `logging` module with configurable levels~~ âœ…

## ğŸ”µ Missing Features

- ~~**No robots.txt compliance** â€” Crawler doesn't check if crawling is allowed by the website â†’ Add `urllib.robotparser` to check robots.txt before crawling any domain~~ âœ…

- ~~**No rate limiting in crawler** â€” Crawler fires requests as fast as possible, could get IP banned â†’ Add `time.sleep(0.5)` between requests minimum~~ âœ…

- **No crawl progress feedback** â€” User has no idea how crawl is progressing â†’ Add WebSocket or polling endpoint to show crawl progress in real-time

- ~~**No error handling for failed URLs** â€” Crawler silently skips failed requests with no logging â†’ Add proper error logging and optional retry mechanism~~ âœ…

- **No graph export feature** â€” User cannot download the computed PageRank results â†’ Add "Export as JSON/CSV" button for results

- ~~**No node filtering in visualization** â€” With 100+ nodes, graph is cluttered â†’ Add slider to filter by minimum PageRank score or degree~~ âœ…

## ğŸŸ¢ Code Quality

- **No TypeScript** â€” Frontend is plain JavaScript with no type safety â†’ Convert to TypeScript for better maintainability

- **No linting configured** â€” No ESLint or Prettier setup for consistent code style â†’ Add `.eslintrc` and `.prettierrc` with pre-commit hooks

- **Missing JSDoc comments** â€” Functions lack documentation â†’ Add JSDoc comments to all exported functions

- **No environment variables** â€” Backend port and URLs are hardcoded â†’ Use `.env` files for configuration

- **Package versions not locked** â€” Using `^` versions allows breaking changes â†’ Use exact versions or `package-lock.json` in git

## ğŸ“ Documentation

- **README lacks setup instructions** â€” Doesn't explain how to run Python crawler or that you need both backend and frontend â†’ Add step-by-step setup guide for all components

- **No API documentation** â€” Backend endpoints are undocumented â†’ Add OpenAPI/Swagger spec or at least document in README

- **No architecture diagram** â€” Hard to understand how pieces fit together â†’ Add simple diagram showing crawler â†’ backend â†’ frontend flow

## ğŸ§ª Testing

- **Zero backend tests** â€” No tests for PageRank algorithm or API endpoints â†’ Add Jest tests for `pagerank.js` and supertest for API routes

- **Zero Python tests** â€” No tests for crawler or Python PageRank â†’ Add pytest tests for `crawler.py` and `pagerank_calc.py`

- **No CI/CD pipeline** â€” Tests don't run automatically on push â†’ Add GitHub Actions workflow for linting and testing

---

## Priority Order

1. Fix `run_pagerank.py` syntax error
2. Delete duplicate files and empty placeholders
3. Connect frontend to backend
4. Add basic tests
5. Add rate limiting to crawler
6. Everything else
